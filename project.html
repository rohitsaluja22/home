<!DOCTYPE html>
<head>
	 <meta charset="utf-8">
  <link href="/normalize.css" rel="stylesheet">
  <style>
    header {
		
      text-align: center;
      background: url("background.png");
      background-size: cover;
	   background-attachment: fixed;
      color: white;
	 
	  
    }
	div{
	font-family:Tahoma, Geneva, sans-serif;
	display:inline;
	
	}
	body{
	 margin:0px;
	 padding:0px;
	background-image:url("back.png")
	}
	p {
    font-family:Tahoma, Geneva, sans-serif;
	text-align: justify;
	}
	h2{
	font-family:Tahoma, Geneva, sans-serif;
	color:Tomato;
	text-align:center;
	font-size:30px;
	}
    a {
      color: white;
	  padding:10px;
	  
	}
	.diff{
	color:black;
	}
	a.nav:hover {
    background-color:green;
	  }
	a.new {
      color: blue;
	  }
	  
    h1 {
      font-size: 70px;
    }
	
    ul {
		
		font-family:Tahoma, Geneva, sans-serif;
      padding: 10px;
      background: rgba(0,0,0,0.5);
	  attachment:s;
    }
	
    li {
      display: inline;
	    }
	
    .float {
		max-width: 885px;
		padding: 20px;
        margin: auto;
		text-align:justify;
		font-family:Tahoma, Geneva, sans-serif;
      }
      @media (max-width: 500px){
       h1{
         font-size: 36px;
         }
        
        body {
         
		 
          }
        }
		    .img-circle {
		margin-top: 40px;
        border-radius: 25%;
		border: 7px solid rgba(0,0,0,0.5);
		
    }
  </style>
</head>
<body>
  <header>
  <img class="img-circle" src="pics\rohit.png" max-width="150px" height="180px">
    <h1 style="color:white;letter-spacing: 10px;text-shadow: 5px 2px rgba(0,0,0,0.8);">ROHIT SALUJA</h1>
    <ul>
      		 <li><a class="nav" href="index.html" style="text-decoration:none">HOME</a></li>
    		 <li><a class="nav" href="project.html" style="text-decoration:none">PROJECTS</a></li>
	 	 <li><a class="nav" href="publication.html" style="text-decoration:none">PUBLICATIONS & AWARDS</a></li>
		 <li><a class="nav" href="cv.html" style="text-decoration:none">ABOUT ME</a></li>
	
    </ul>
  </header>
  <h2>Road Safety and Robust multilingual OCR:<h2>

  <h2 style="color:tomato" class="float">Road Safety:</h2>
<article class="float">
	Automated road surveillance has become increasingly crucial as road crashes have become the $8^{th}$ leading cause of death worldwide. A World Health Organization study on road safety claims that violations lead to $1.35$ million in fatalities and affect $50$ million people yearly. Another recent report by  World Bank (2021) mentions that more than $50\%$ of road fatalities involve two-wheeler vehicles, also showing that `no helmet' and triple-riding (more than two riders) violations are common causes. Studies carried out in Asian countries also account for two-wheeler vehicles among the significant share of road fatalities. Motivated by the worldwide need to regularly update research on road safety, we work on problems like motorcycle violations detection, counting street trees, open-world object detection, domain adaptation, and self-supervised image deraining in autonomous space.
<br>	
<p>
			<strong>1.  CVPR UG2+ Workshop 2022: Motorcycle Violations</strong>
			<br><br>				
			<p>		a. You can read the paper<a style="color:blue" href="CVPRW_MotorcycleViolations.pdf">here</a></p>	
			<p>		b. Source Code for the paper is <a class="new" href="https://github.com/iHubData-Mobility/public-motorcycle-violations">here</a></p>	
			<p>		c. Demo video: <iframe width="420" height="345" src="https://www.youtube.com/watch?v=ypqGihjh-CQ">
							</iframe></p> 
		<br>
	</p>
	<p>
			<strong>2.  ICVGIP 2019: Quantification and Visualization of Street Trees</strong>
			<br><br>
		<p>	a. You can read the paper<a class="new" href="http://cvit.iiit.ac.in/images/ConferencePapers/2021/Automatic_tree.pdf">here</a></p>
		<p>	b. Demo video: <iframe width="420" height="345" src="https://youtu.be/fA4K3tYMTGs">
							</iframe></p>
		<p>	b. Source code for our framework is available<a class="new" href="https://github.com/iHubData-Mobility/public-tree-counting">here</a></p>
	</p>
</article>
  <h2 style="color:tomato" class="float">IndicOCR:</h2>
<article class="float">
	Optical Character Recognition (OCR) is the process of converting the document images into an editable electronic format. This has many advantages like data compression, enabling search or edit options in the images/text, and creating the database for other applications like Machine Translation, Speech Recognition, and enhancing dictionaries and language models.

	OCR in Indian Languages is quite challenging due to richness in inflections.<br><br> Using Open Source and Commercial OCR systems, we have observed the Word Error Rates (WER) of around 20-50% on printed documents in four different Indic languages. Moreover, developing a highly accurate OCR system with an accuracy as high as 90% is not useful unless aided by the mechanism to identify errors. So, we started with the problem of developing <strong> "OpenOCRCorrect"</strong>, an end-to-end framework for Error Detection and Corrections in Indic-OCR. Our models outperform state-of-the-art results in <strong>“Error Detection in Indic-OCR” </strong>for six Indic languages with varied inflections and we have solved the Out of Vocabulary problem for <strong> “Error Correction in Indic-OCR” </strong> in our ICDAR-2017 conference paper. We further improve the results with the help of sub-word embeddings in our ICDAR-2019 conference paper.
<br>
<p>
			<strong>1.  ICDAR 2019 Post-OCR Competetion:</strong>
			<br><br>				
			<p>		a. Our team "CLAM" secured 2nd position in Multilingual PostOCR Competetion at ICDAR'19. Our model achieved highest corrections of 44% in Finnish, which is significantly higher than overall topper (8% in Finnish). Final <a style="color:blue" href="https://drive.google.com/open?id=15mxNO-M9PiXBnffi7MOa8wUw33nj1xBp">report</a> and <a style="color:blue" href="https://drive.google.com/open?id=1uuBWu1LQ1QZ49SCgLBoB1er4HpWSzmcx">poster</a> available.</p>	
		<br>
	</p>
	<p>
			<strong>2.  ICDAR2019:</strong>
			<br><br>				
			<p>		a. You can read the paper<a style="color:blue" href="PID6011473.pdf">here</a></p>	
		<br>
	</p>
	<p>
			<strong>3.  ICDAR2017:</strong>
			<br><br>				
			<p>		a. You can read the paper<a class="new" href="https://ieeexplore.ieee.org/document/8269944">here</a></p>	
			<p>		b. Dataset can be requested via email on<a class="new"  href="rohitsaluja22@gmail.com">rohitsaluja22@gmail.com</a></p>	
		<br>
	</p>
	<p>
			<strong>4.  ICDAR-OST 2017: OpenOCRCorrect</strong>
			<br><br>
		<p>	a. You can read the paper<a class="new" href="https://ieeexplore.ieee.org/abstract/document/8270254/">here</a></p>
		<p>	b. Demo video for our framework is<a class="new" href="https://drive.google.com/file/d/0B23BxAny5xHKc3FweEQ5b2gxNUk/view">here</a></p>
		<p>	c. Source code for our framework is available<a class="new" href="https://github.com/rohitsaluja22/OpenOCRCorrect">here</a></p>
	
	
	</p>
</article>
  <h2 style="color:tomato" class="float">OCR On-the-go:</h2>
<article class="float">
	We work on the problem of recognizing license plates and street signs automatically, particularly in challenging conditions such as chaotic traffic. We leverage state-of-the-art text spotters to generate a large amount of noisy labeled training data. The data is subsequently filtered using a pattern derived from domain knowledge. We augment training and testing data with interpolated boxes and annotations which makes our training and testing robust. We further use synthetic data during training to increase the coverage of the training data. We trained two different models for recognition. Our baseline is a conventional Convolution Neural Network (CNN) as the encoder followed by a Recurrent Neural Network (RNN) decoder. As our first contribution, we bypass the detection phase by augmenting the baseline with an Attention mechanism in the RNN decoder. Next, we build in the capability of training the model end-to-end on scenes containing license plates by incorporating inception based CNN encoder that makes the model robust to multiple scales. We achieve improvements of as large as 3.75% at the sequence level, over the baseline model. We present the first results of using multi-headed attention models on text recognition in images and illustrate the advantages of using multiple-heads over a single head. We observe even more gains as large as 7.18% by incorporating multi-headed attention. We also experiment with multi-headed attention models on French Street Name Signs dataset (FSNS) and a new Indian Street dataset that we release for experiments. We observe that such models with multiple attention masks perform better than the model with single-headed attention on three different datasets with varying complexities. Our models also outperform state-of-the-art results on FSNS dataset and IIIT-ILST Devanagari dataset. 
<br>	
<p>
			<strong>1.  ICDAR2019:</strong>
			<br><br>				
			<p>		a. You can read the paper<a style="color:blue" href="PID6011503.pdf">here</a></p>	
			<p>		b. Source Code for the paper is <a class="new" href="https://github.com/rohitsaluja22/OCR-On-the-go">here</a></p>	
			<p>		c. Dataset can be requested via email on<a class="new"  href="rohitsaluja@cse.iitb.ac.in">rohitsaluja@cse.iitb.ac.in</a></p> 
			<p>		d. Demo video for our ALPR model is<a class="new"  href="https://drive.google.com/open?id=1xetAEINYOlS-HUgFj71DRMtdr8FMVBtc">here</a></p> 
		<br>
	</p>
	<p>
			<strong>2.  ICDAR-OST 2019: StreetOCRCorrect</strong>
			<br><br>
		<p>	a. You can read the paper<a class="new" href="https://www.cse.iitb.ac.in/~ganesh/papers/icdarost2019.pdf">here</a></p>
		<p>	b. Demo video for our framework is<a class="new" href="https://drive.google.com/file/d/17geb-J9TAhOWGEn754X3VTFf4cveOzIW/view">here</a></p>
		<p>	c. Source code for our framework is available<a class="new" href="https://github.com/rohitsaluja22/StreetOCRCorrect">here</a></p>
	</p>
</article>
 <footer>
 <p style="text-align:center;background-color:rgba(0,0,0,0.5);padding:7px;margin:0px;">&copy copyright reserved by Rohit Saluja.</p>
 </footer>

</body>
