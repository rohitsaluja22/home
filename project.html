<!DOCTYPE html>
<head>
	 <meta charset="utf-8">
  <link href="/normalize.css" rel="stylesheet">
  <style>
    header {
		
      text-align: center;
      background: url("background.png");
      background-size: cover;
	   background-attachment: fixed;
      color: white;
	 
	  
    }
	div{
	font-family:Tahoma, Geneva, sans-serif;
	display:inline;
	
	}
	body{
	 margin:0px;
	 padding:0px;
	background-image:url("back.png")
	}
	p {
    font-family:Tahoma, Geneva, sans-serif;
	text-align: justify;
	}
	h2{
	font-family:Tahoma, Geneva, sans-serif;
	color:Tomato;
	text-align:center;
	font-size:30px;
	}
    a {
      color: white;
	  padding:10px;
	  
	}
	.diff{
	color:black;
	}
	a.nav:hover {
    background-color:green;
	  }
	a.new {
      color: blue;
	  }
	  
    h1 {
      font-size: 70px;
    }
	
    ul {
		
		font-family:Tahoma, Geneva, sans-serif;
      padding: 10px;
      background: rgba(0,0,0,0.5);
	  attachment:s;
    }
	
    li {
      display: inline;
	    }
	
    .float {
		max-width: 885px;
		padding: 20px;
        margin: auto;
		text-align:justify;
		font-family:Tahoma, Geneva, sans-serif;
      }
      @media (max-width: 500px){
       h1{
         font-size: 36px;
         }
        
        body {
         
		 
          }
        }
		    .img-circle {
		margin-top: 40px;
        border-radius: 25%;
		border: 7px solid rgba(0,0,0,0.5);
		
    }
  </style>
</head>
<body>
  <header>
  <img class="img-circle" src="pics\rohit.jpeg" max-width="150px" height="180px">
    <h1 style="color:white;letter-spacing: 10px;text-shadow: 5px 2px rgba(0,0,0,0.8);">ROHIT SALUJA</h1>
    <ul>
      		 <li><a class="nav" href="index.html" style="text-decoration:none">HOME</a></li>
    		 <li><a class="nav" href="project.html" style="text-decoration:none">PROJECTS</a></li>
    		 <li><a class="nav" href="publication.html" style="text-decoration:none">PUBLICATIONS</a></li>
    		 <li><a class="nav" href="awards.html" style="text-decoration:none">AWARDS</a></li>
		 <li><a class="nav" href="cv.html" style="text-decoration:none">ABOUT ME</a></li>
	
    </ul>
  </header>
  <h2>Road Safety and Robust multilingual OCR:<h2>

  <h2 style="color:tomato" class="float">Road Safety:</h2>
<article class="float">
	Automated road surveillance has become increasingly crucial as road crashes have become the 8<sup>th</sup> leading cause of death worldwide. A World Health Organization study (2018) on road safety claims that violations lead to 1.35 million in fatalities and affect 50 million people yearly. Another recent report by  World Bank (2021) mentions that more than 50 % of road fatalities involve two-wheeler vehicles, also showing that no helmet and triple-riding (more than two riders) violations are common causes. Studies carried out in Asian countries also account for two-wheeler vehicles among the significant share of road fatalities. Motivated by the worldwide need to regularly update research on road safety, we work on problems like counting motorcycle violations and street trees, open-world object detection, self-supervised image deraining, domain adaptation, and incremental learning in the field of autonomous navigation.
<br>	
<p>
			<strong>1.  CVPR, UG2+ 2022: Detecting, Tracking and Counting Motorcycle Rider Traffic Violations on Unconstrained Roads</strong>
			<br><br>				
			<p>		a. You can read the paper<a style="color:blue" href="CVPRW_MotorcycleViolations.pdf">here</a></p>	
			<p>		b. Source code: <a class="new" href="https://github.com/iHubData-Mobility/public-motorcycle-violations">here</a></p>	
			<p>		c. Demo video: <iframe width="614" height="345" src="https://www.youtube.com/embed/ypqGihjh-CQ">
							</iframe></p> 
		<br>
	</p>
	<p>
			<strong>2. WACV 2022: "FLUID: Few-Shot Self-Supervised Image Deraining" </strong>
			<br><br>
		<p>	a. You can read the paper<a class="new" href="https://openaccess.thecvf.com/content/WACV2022/papers/Nandan_FLUID_Few-Shot_Self-Supervised_Image_Deraining_WACV_2022_paper.pdf">here</a></p>
		<p>	b. Demo video coming soon.
	</p>
	<p>
			<strong>3. WACV 2022: To miss-attend is to misalign! Residual Self-Attentive Feature Alignment for Adapting Object Detectors</strong>
			<br><br>
		<p>	a. You can read the paper<a class="new" href="https://openaccess.thecvf.com/content/WACV2022/papers/Khindkar_To_Miss-Attend_Is_to_Misalign_Residual_Self-Attentive_Feature_Alignment_for_WACV_2022_paper.pdf">here</a></p>
		<p>	b. Demo video coming soon.
	</p>
	<p>
			<strong>4. WACV 2022: Multi-Domain Incremental Learning for Semantic Segmentation</strong>
			<br><br>
		<p>	a. You can read the paper<a class="new" href="https://openaccess.thecvf.com/content/WACV2022/papers/Garg_Multi-Domain_Incremental_Learning_for_Semantic_Segmentation_WACV_2022_paper.pdf">here</a></p>
		<p>	b. Demo video coming soon.
	</p>
	<p>
			<strong>5.  NeurIPS, Machine Learning for Autonomous Driving 2021: "ORDER: Open World Object Detection on Road Scenes"</strong>
			<br><br>
		<p>	a. You can read the paper<a class="new" href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2021/ORDER-Open-World-Object.pdf">here</a></p>
		<p>	b. Demo video coming soon
	</p>
	<p>
	<p>
			<strong>6.  arXiv 2021: Evaluating Computer Vision Techniques for Urban Mobility on Large-Scale, Unconstrained Roads</strong>
			<br><br>
		<p>	a. You can read the paper<a class="new" href="https://arxiv.org/pdf/2109.05226.pdf">here</a></p>
		<p>	b. Demo video coming soon
	</p>
	<p>
			<strong>7.  ICVGIP 2021: Automatic Quantification and Visualization of Street Trees</strong>
			<br><br>
		<p>	a. You can read the paper<a class="new" href="http://cvit.iiit.ac.in/images/ConferencePapers/2021/Automatic_tree.pdf">here</a></p>
		<p>	b. Source code: <a class="new" href="https://github.com/iHubData-Mobility/public-tree-counting">here</a></p>
		<p>	c. Demo video: <iframe width="614" height="345" src="https://www.youtube.com/embed/fA4K3tYMTGs">
							</iframe></p>
	</p>
</article>
  <h2 style="color:tomato" class="float">Photo OCR:</h2>
<article class="float">
	We work on the problem of recognizing license plates and street signs automatically, particularly in challenging conditions such as chaotic traffic. We leverage state-of-the-art text spotters to generate a large amount of noisy labeled training data. The data is subsequently filtered using a pattern derived from domain knowledge. We augment training and testing data with interpolated boxes and annotations which makes our training and testing robust. We further use synthetic data during training to increase the coverage of the training data. We trained two different models for recognition. Our baseline is a conventional Convolution Neural Network (CNN) as the encoder followed by a Recurrent Neural Network (RNN) decoder. As our first contribution, we bypass the detection phase by augmenting the baseline with an Attention mechanism in the RNN decoder. Next, we build in the capability of training the model end-to-end on scenes containing license plates by incorporating inception based CNN encoder that makes the model robust to multiple scales. We achieve improvements of as large as 3.75% at the sequence level, over the baseline model. We present the first results of using multi-headed attention models on text recognition in images and illustrate the advantages of using multiple-heads over a single head. We observe even more gains as large as 7.18% by incorporating multi-headed attention. We also experiment with multi-headed attention models on French Street Name Signs dataset (FSNS) and a new Indian Street dataset that we release for experiments. We observe that such models with multiple attention masks perform better than the model with single-headed attention on three different datasets with varying complexities. Our models also outperform state-of-the-art results on FSNS dataset and IIIT-ILST Devanagari dataset. 
<br>
	<p>
			<strong>1.  CBDAR 2021 Best Paper and MDPI Journal of Imaging 2022: Improving Scene Text Recognition for Indian Languages with Transfer Learning and Font Diversity</strong>
			<br><br>
		<p>	a. You can read the papers here: <a class="new" href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2021/Transfer_Learning_Indian_STR.pdf">CBDAR</a> and <a class="new" href="https://www.mdpi.com/2313-433X/8/4/86/pdf">MDPI</a></p>
		<p>	b. Dataset: <a class="new" href="http://cvit.iiit.ac.in/research/projects/cvit-projects/transfer-learning-for-scene-text-recognition-in-indian-languages">here</a></p>
		<p>	c. Source code: <a class="new" href="https://github.com/firesans/STRforIndicLanguages">here</a></p>
	</p>
	<p>
			<strong>2.  CBDAR 2021: CATALIST: CAmera TrAnsformations for multi-LIngual Scene Text recognition</strong>
			<br><br>
		<p>	a. You can read the paper <a class="new" href="https://www.cse.iitb.ac.in/~ganesh/papers/catalist2021.pdf">here</a>
		<p>	b. Dataset: <a class="new" href="https://catalist-2021.github.io/">here</a></p>
	</p>
	<p>
			<strong>3.  ICDAR2019: OCR-On-the-go</strong>
			<br><br>				
			<p>		a. You can read the paper<a style="color:blue" href="https://www.cse.iitb.ac.in/~rohitsaluja/PID6011503.pdf">here</a></p>	
			<p>		b. Source Code for the paper is <a class="new" href="https://github.com/rohitsaluja22/OCR-On-the-go">here</a></p>	
			<p>		c. Dataset can be requested via email on<a class="new"  href="rohitsaluja22@gmail.com">rohitsaluja22@gmail.com</a></p> 
			<p>		d. Demo video for our ALPR model is<a class="new"  href="https://drive.google.com/open?id=1xetAEINYOlS-HUgFj71DRMtdr8FMVBtc">here</a></p> 
		<br>
	</p>
	<p>
			<strong>4.  ICDAR-OST 2019: StreetOCRCorrect</strong>
			<br><br>
		<p>	a. You can read the paper<a class="new" href="https://www.cse.iitb.ac.in/~ganesh/papers/icdarost2019.pdf">here</a></p>
		<p>	b. Demo video for our framework is<a class="new" href="https://drive.google.com/file/d/17geb-J9TAhOWGEn754X3VTFf4cveOzIW/view">here</a></p>
		<p>	c. Source code for our framework is available<a class="new" href="https://github.com/rohitsaluja22/StreetOCRCorrect">here</a></p>
	</p>
</article>
  <h2 style="color:tomato" class="float">Document OCR:</h2>
<article class="float">
	Optical Character Recognition (OCR) is the process of converting the document images into an editable electronic format. This has many advantages like data compression, enabling search or edit options in the images/text, and creating the database for other applications like Machine Translation, Speech Recognition, and enhancing dictionaries and language models.

	OCR in Indian Languages is quite challenging due to richness in inflections.<br><br> Using Open Source and Commercial OCR systems, we have observed the Word Error Rates (WER) of around 20-50% on printed documents in four different Indic languages. Moreover, developing a highly accurate OCR system with an accuracy as high as 90% is not useful unless aided by the mechanism to identify errors. So, we started with the problem of developing <strong> "OpenOCRCorrect"</strong>, an end-to-end framework for Error Detection and Corrections in Indic-OCR. Our models outperform state-of-the-art results in <strong>“Error Detection in Indic-OCR” </strong>for six Indic languages with varied inflections and we have solved the Out of Vocabulary problem for <strong> “Error Correction in Indic-OCR” </strong> in our ICDAR-2017 conference paper. We further improve the results with the help of sub-word embeddings in our ICDAR-2019 conference paper.
<br>
	<p>
			<strong>1.  CVPRW 2020: An OCR for Classical Indic Documents Containing Arbitrarily Long Words</strong>
			<br><br>				
			<p>		a. You can read the paper<a style="color:blue" href="http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2020/OCR_Classical_Indic_Documents_Containing.pdf">here</a></p>
			<p>	c. Source code: <a class="new" href="https://github.com/ihdia/sanskrit-ocr">here</a></p>
			<p>	c. Demo video coming soon
		<br>
	</p>
	<p>
	<p>
			<strong>2.  ICDAR 2019: Post-OCR Competetion</strong>
			<br><br>				
			<p>		a. Our team "CLAM" secured 2nd position in Multilingual PostOCR Competetion at ICDAR'19. Our model achieved highest corrections of 44% in Finnish, which is significantly higher than overall topper (8% in Finnish). Final <a style="color:blue" href="https://drive.google.com/open?id=15mxNO-M9PiXBnffi7MOa8wUw33nj1xBp">report</a> and <a style="color:blue" href="https://drive.google.com/open?id=1uuBWu1LQ1QZ49SCgLBoB1er4HpWSzmcx">poster</a> available.</p>	
		<br>
	</p>
	<p>
			<strong>3.  ICDAR2019: Sub-word Embeddings for OCR Corrections in Highly Fusional Indic Languages</strong>
			<br><br>				
			<p>		a. You can read the paper<a style="color:blue" href="https://www.cse.iitb.ac.in/~ganesh/papers/icdar2019a.pdf">here</a></p>	
		<br>
	</p>
	<p>
			<strong>4.  ICDAR2017: Error Detection and Corrections in Indic OCR using LSTMs</strong>
			<br><br>				
			<p>		a. You can read the paper<a class="new" href="https://www.cse.iitb.ac.in/~ganesh/papers/icdar17.pdf">here</a></p>	
			<p>		b. Dataset can be requested via email on<a class="new"  href="rohitsaluja22@gmail.com">rohitsaluja22@gmail.com</a></p>	
		<br>
	</p>
	<p>
			<strong>5.  ICDAR-OST 2017: OpenOCRCorrect</strong>
			<br><br>
		<p>	a. You can read the paper<a class="new" href="https://ieeexplore.ieee.org/abstract/document/8270254/">here</a></p>
		<p>	b. Source code for our framework is available<a class="new" href="https://github.com/rohitsaluja22/OpenOCRCorrect">here</a></p>
		<p>	c. Demo video: <iframe width="614" height="345" src="https://www.youtube.com/embed/iYagbg-yKsc">
							</iframe></p> 
	
	
	
	</p>
</article>
 <footer>
 <p style="text-align:center;background-color:rgba(0,0,0,0.5);padding:7px;margin:0px;">&copy copyright reserved by Rohit Saluja.</p>
 </footer>

</body>
